This project implements a Variational Autoencoder (VAE) for unsupervised anomaly detection
and compares its performance against a baseline Autoencoder.

Dataset:
MNIST (784-dimensional).
Digit 0 treated as normal; digits 1–9 treated as anomalies.
Training performed only on normal samples.

Model Architecture:
Encoder: 784 → 512 → 256 → latent
Decoder: latent → 256 → 512 → 784

Hyperparameter Search:
Latent dimensions tested: 8, 16, 32
Beta values tested: 0.5, 1.0, 2.0

A validation split (20%) of normal data was used.
The configuration (latent=__ , beta=__) achieved the lowest validation reconstruction error
and was selected.

Threshold Selection:
The anomaly threshold was defined as the 95th percentile of reconstruction error
on training normal samples.

Results:

VAE:
Precision: ___
Recall: ___
F1 Score: ___

Baseline Autoencoder:
Precision: ___
Recall: ___
F1 Score: ___

Analysis:
The VAE consistently outperformed the baseline Autoencoder.
KL divergence regularizes the latent space, improving generalization
and reducing overfitting to normal samples.
This leads to better anomaly separation.

Conclusion:
The experiment demonstrates that Variational Autoencoders provide
a more robust framework for anomaly detection than standard Autoencoders.
