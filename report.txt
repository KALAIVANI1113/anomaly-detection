Project: Variational Autoencoder for Anomaly Detection

Dataset:
MNIST dataset (784-dimensional input).
Digit 0 treated as normal.
Digits 1–9 treated as anomalies.
Training performed only on normal samples.

Model Architecture:

VAE:
Input → 400 (ReLU) → Latent (16)
Latent sampled using reparameterization trick.
Decoder mirrors encoder.

Loss:
Total = Reconstruction (MSE) + β * KL Divergence
β = 1.0

Hyperparameter Tuning:

Latent Dimension tested: 8, 16, 32
16 provided best reconstruction–generalization balance.

Beta tested: 0.5, 1.0, 2.0
1.0 provided stable convergence and effective latent regularization.

Threshold:
95th percentile of training reconstruction error.

Final Results:

VAE:
Precision: ______
Recall: ______
F1 Score: ______

Baseline Autoencoder:
Precision: ______
Recall: ______
F1 Score: ______

Analysis:

The VAE outperformed the baseline Autoencoder due to KL regularization,
which structures the latent space and improves generalization.

The baseline Autoencoder lacks probabilistic latent modeling,
which can lead to overfitting on normal samples.

Conclusion:

The VAE successfully modeled the normal data distribution and
detected anomalies using reconstruction error thresholding.
The comparison demonstrates the advantage of variational modeling
for anomaly detection.
